{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess widefield calcium imaging data using Spark\n",
    "This notebook demonstrates how to read binary raw data files stored on UZH Swift object storage into a Spark RDD, convert it into a Numpy array and perform preprocessing to generate a DFF array. Both the raw data and DFF arrays are stored as output HDF5 files on the Swift object storage. **It's setup to test different parameters (e.g. Spark number of cores) in a loop.** For details and better documentation see the notebook Widefield_Preproc_Spark_Swift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from __future__ import print_function\n",
    "import getpass\n",
    "import h5py\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# the notebook backend: 'local' or 'openstack'\n",
    "nbBackend = 'openstack'\n",
    "\n",
    "# add folder 'utils' to the Python path\n",
    "# this folder contains custom written code that is required for data import and analysis\n",
    "utils_dir = os.path.join(os.getcwd(), 'utils')\n",
    "sys.path.append(utils_dir)\n",
    "\n",
    "# Import custom-written modules\n",
    "import SwiftStorageUtils\n",
    "import WidefieldDataUtils as wf\n",
    "import PickleUtils as pick\n",
    "import CalciumAnalysisUtils as calciumTools\n",
    "import parseDCIMGheader as parseDCIMGheader\n",
    "from SwiftStorageUtils import saveAsH5\n",
    "from SwiftStorageUtils import deleteExistingFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start of name for matching files\n",
    "filename_start = '20161602_' # all files with names starting like this will be processed\n",
    "\n",
    "# swift file system\n",
    "swift_container = 'paramTest' # specify name of container in Swift (do not use _ etc. in container names!)\n",
    "swift_provider = 'SparkTest' # in general, this should not change\n",
    "\n",
    "# derive the Swift base URI\n",
    "swift_basename = \"swift://\" + swift_container + \".\" + swift_provider + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OpenStack credentials for accessing Swift storage\n",
    "os_username = 'hluetc'\n",
    "os_tenant_name = 'helmchen.hifo.uzh'\n",
    "os_auth_url = 'https://cloud.s3it.uzh.ch:5000/v2.0'\n",
    "# provide OS password\n",
    "os_password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put all these parameters in a dictionary, so that we can pass them conveniently to functions\n",
    "file_params = dict()\n",
    "file_params['filename_start'] = filename_start\n",
    "file_params['swift_container'] = swift_container\n",
    "file_params['swift_provider'] = swift_provider\n",
    "file_params['swift_basename'] = swift_basename\n",
    "file_params['os_username'] = os_username\n",
    "file_params['os_tenant_name'] = os_tenant_name\n",
    "file_params['os_auth_url'] = os_auth_url\n",
    "file_params['os_password'] = os_password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment & analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image dimensions for analysis (aspect ratio MUST be preserved)\n",
    "dims_analysis = (256,256) # use None to skip resizing\n",
    "\n",
    "# sampling rate and trial times\n",
    "sample_rate = 20.0 # Hz\n",
    "t_stim = -1.9 # stimulus cue (auditory)\n",
    "t_textIn = 0 # texture in (i.e. stimulus onset)\n",
    "t_textOut = 2 # texture starting to move out (stimulus offset)\n",
    "t_response = 4.9 # response cue for licking (auditory)\n",
    "t_base = -2 # baseline end (for F0 calculation)\n",
    "\n",
    "# Analysis parameters\n",
    "bg_smooth = 30 # SD of Gaussian smoothing kernel for background estimation (in pixel) \n",
    "\n",
    "seg_cutoff = 0.0002 # Segmentation threshold; larger value = bigger mask; \n",
    "# smaller value = smaller mask (i.e. more pixels ignored); suggested = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define required functions\n",
    "Note that this approach only works in the notebook because variables are globally defined. If this code is ported to a standalone Python app, all variables need to be passed explicitely to the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from setupSpark import initSpark\n",
    "\n",
    "# # spark_instances = 2 # the number of workers to be used\n",
    "# executor_cores = 8 # the number of cores to be used on each worker\n",
    "# executor_memory = '25G' # the amount of memory to be used on each worker\n",
    "# max_cores = 8 # the max. number of cores Spark is allowed to use overall\n",
    "\n",
    "# sc = initSpark(nbBackend, executor_cores=executor_cores, \\\n",
    "#                max_cores=max_cores, executor_memory=executor_memory)\n",
    "\n",
    "# # provide OpenStack credentials to the Spark Hadoop configuration\n",
    "# sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.username', os_username)\n",
    "# sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.tenant', os_tenant_name)\n",
    "# sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.password', os_password)\n",
    "\n",
    "# # add Python files in 'utils' folder to the SparkContext \n",
    "# # this is required so that all files are available on all the cluster workers\n",
    "# for filename in os.listdir(utils_dir):\n",
    "#     if filename.endswith('.py'):\n",
    "#         sc.addPyFile(os.path.join(utils_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def startSparkContext(max_cores=16):\n",
    "    from setupSpark import initSpark\n",
    "    \n",
    "#     spark_instances = spark_instances # the number of workers to be used\n",
    "    executor_cores = 8 # the number of cores to be used on each worker\n",
    "    executor_memory = '25G' # the amount of memory to be used on each worker\n",
    "    max_cores = max_cores # the max. number of cores Spark is allowed to use overall\n",
    "\n",
    "    # returns the SparkContext object 'sc' which tells Spark how to access the cluster\n",
    "    sc = initSpark(nbBackend, executor_cores=executor_cores, \\\n",
    "                   max_cores=max_cores, executor_memory=executor_memory)\n",
    "    \n",
    "    # provide OpenStack credentials to the Spark Hadoop configuration\n",
    "    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.username', os_username)\n",
    "    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.tenant', os_tenant_name)\n",
    "    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.password', os_password)\n",
    "    \n",
    "    # add Python files in 'utils' folder to the SparkContext \n",
    "    # this is required so that all files are available on all the cluster workers\n",
    "    for filename in os.listdir(utils_dir):\n",
    "        if filename.endswith('.py'):\n",
    "            sc.addPyFile(os.path.join(utils_dir, filename))\n",
    "            \n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertDCAMtoMov(byte_stream):\n",
    "    \"\"\"\n",
    "    Convert raw DCAM byte-stream to movie. \n",
    "    \n",
    "    Note that parameters (e.g. dims_analysis) are provided as global variables in the notebook.\n",
    "    Image dimensions are obtained by parsing the file header.\n",
    "    \"\"\"\n",
    "    # parse the header and get image dimensions\n",
    "    hdr = parseDCIMGheader.main(byte_stream)\n",
    "    dims = [hdr['xsize'], hdr['ysize'], hdr['nframes']]\n",
    "    \n",
    "    byte_stream = byte_stream[232:] # 232 bytes is the file header\n",
    "    A = np.fromstring(byte_stream, dtype=np.uint16)\n",
    "    A = A[:dims[0]*dims[1]*dims[2]] # remove data points at the end\n",
    "    \n",
    "    # re-arrange data into the correct shape\n",
    "    mov = np.fliplr(A.reshape([dims[0], dims[1], dims[2]], order='F'))\n",
    "    # hack to remove strange pixels with very high intensity\n",
    "    mov[np.where(mov > 60000)] = 0\n",
    "    \n",
    "    # resize to analysis dimensions\n",
    "    mov = wf.resizeMovie(mov, resolution=dims_analysis, interp='bilinear')\n",
    "    \n",
    "    return mov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocMovie(mov, bg_smooth=bg_smooth, seg_cutoff=seg_cutoff):\n",
    "    \"\"\"\n",
    "    Perform preprocessing steps for a movie. \n",
    "    \"\"\"\n",
    "    \n",
    "    # estimate background signal intensity\n",
    "    print('Estimating background', end=\"\")\n",
    "    bg_estimate = wf.estimateBackground(mov[:,:,0], bg_smooth)\n",
    "    print(' - Done (%1.2f)' % bg_estimate)\n",
    "    \n",
    "    # subtract the background (set negative to 0)\n",
    "    mov = mov - bg_estimate\n",
    "    mov[mov<0] = 0\n",
    "    \n",
    "    # segment out the background (set to np.nan)\n",
    "    print('Segmenting background', end=\"\")\n",
    "    mov = wf.segmentBackground(mov, seg_cutoff, plot=False)\n",
    "    print(' - Done')\n",
    "    \n",
    "    # baseline normalization (Dff)\n",
    "    print('Calculating Dff', end=\"\")\n",
    "    dff = calciumTools.calculateDff(mov , f0_frames)\n",
    "    print(' - Done')\n",
    "    \n",
    "    return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFileNameFromKey(key):\n",
    "    \"\"\"\n",
    "    Return the file name from the RDD key (i.e. split of the swift URL)\n",
    "    \"\"\"\n",
    "    path, name = os.path.split(key)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setupRDDs(sc, file_params):\n",
    "    file_rdd = sc.binaryFiles(file_params['swift_basename'])\n",
    "    file_rdd = file_rdd.filter(lambda (k,v): file_params['filename_start'] in k)\n",
    "    # convert byte-stream to movie\n",
    "    mov_rdd = file_rdd.map(lambda (k,v): (k, convertDCAMtoMov(v)))\n",
    "    return (file_rdd, mov_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop to sweep over parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test execution time as a function of number of cores\n",
    "# max_cores_list = range(48,6,-4)\n",
    "max_cores_list = [16]\n",
    "\n",
    "# Set the names for the output folders\n",
    "output_folder_mov = 'mov_out'\n",
    "output_folder_dff = 'dff_out'\n",
    "\n",
    "t_mov = []\n",
    "t_dff = []\n",
    "for i_cores in max_cores_list:\n",
    "    print(\"Cores: %1.0f\" % (i_cores))\n",
    "    sc = startSparkContext(max_cores=i_cores)\n",
    "    time.sleep(20) # wait till setup completes\n",
    "    print(\"Parallelism: %1.0f\" % (sc.defaultParallelism))\n",
    "#     sc.stop()\n",
    "    file_rdd, mov_rdd = setupRDDs(sc, file_params)\n",
    "    if len(t_mov) == 0:\n",
    "        # things that only need to be done once\n",
    "        nTrials = file_rdd.count()\n",
    "        # get first movie (return key-value tuple)\n",
    "        mov1 = mov_rdd.first()\n",
    "        path, file_id = os.path.split(mov1[0])\n",
    "        dat = mov1[1]\n",
    "        timepoints = dat.shape[2]\n",
    "        t = (np.array(range(timepoints)) / sample_rate) - 3.0\n",
    "        # Frames for F0 calculation\n",
    "        f0_frames = t<t_base # F0 as time before baseline\n",
    "        f0_frames[:] = False\n",
    "        f0_frames[9:12] = True # F0 as certain specified frames\n",
    "    \n",
    "    # apply transformation to the RDD\n",
    "    dff_rdd = mov_rdd.map(lambda (k,v): (k, preprocMovie(v)))\n",
    "    \n",
    "    deleteExistingFolder(swift_container, output_folder_mov, file_params, confirm=False)\n",
    "    deleteExistingFolder(swift_container, output_folder_dff, file_params, confirm=False)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # Save the image data as HDF5 on Swift storage. \n",
    "    mov_rdd.foreach(lambda (k,v): (k, saveAsH5(v, getFileNameFromKey(k), 'mov', output_folder_mov, file_params)))\n",
    "    t_mov.append(time.time() - t0)\n",
    "    \n",
    "    print('Cores: %1.0f' % (i_cores))\n",
    "    print('t mov: %1.2f s' % (t_mov[-1]))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # Save the dFF data as HDF5 on Swift storage. \n",
    "    dff_rdd.foreach(lambda (k,v): (k, saveAsH5(v, getFileNameFromKey(k), 'dff', output_folder_dff, file_params)))\n",
    "    t_dff.append(time.time() - t0)\n",
    "    \n",
    "    print('t dff: %1.2f s' % (t_dff[-1]))\n",
    "    \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plot and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "time_str = now.strftime('%Y%m%d-%H%M%S')\n",
    "filename = 'Line_TimePerTrial_vs_nWorker_%1.0f_%s.eps' % (dims_analysis[0], time_str)\n",
    "t_mov_perTrial = [yy/nTrials for yy in t_mov]\n",
    "t_dff_perTrial = [yy/nTrials for yy in t_dff]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(max_instance_list, t_mov_perTrial, 'ks-')\n",
    "plt.plot(max_instance_list, t_dff_perTrial, 'ro-')\n",
    "plt.legend(['mov', 'dff'])\n",
    "plt.xlabel('No. of workers')\n",
    "plt.ylabel('Exec. time per trial / s')\n",
    "plt.title('Trials: %1.0f Resolution: %1.0f' % (nTrials, dims_analysis[0]))\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'Bar_TimePerTrial_vs_nWorker_%1.0f_%s.eps' % (dims_analysis[0], time_str)\n",
    "N = len(max_instance_list)\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "rects1 = ax.bar(ind, t_mov_perTrial, width, color='r')\n",
    "rects2 = ax.bar(ind + width, t_dff_perTrial, width, color='y')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Time per trial / s')\n",
    "ax.set_xlabel('Number of workers')\n",
    "ax.set_title('Trials: %1.0f Resolution: %1.0f' % (nTrials, dims_analysis[0]))\n",
    "xticks = [yy + width for yy in max_instance_list]\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(max_instance_list)\n",
    "plt.legend(('mov', 'dff'), loc='best')\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worker_no = [3, 4, 5, 6]\n",
    "exec_time = [33.34, 26.71, 20.25, 17.78]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "time_str = now.strftime('%Y%m%d-%H%M%S')\n",
    "path = '/Users/Henry/Data/ETH_SIS/eSCT_Project/eSCT_Neuro/2017-01_NeuroSpark_Poster/PerformanceTests'\n",
    "filename = 'Bar_TimePerTrial_vs_nWorker_Corr_%s.eps' % (time_str)\n",
    "\n",
    "N = len(worker_no)\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "rects1 = ax.bar(ind, exec_time, width, color='k')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Time per trial / s')\n",
    "ax.set_xlabel('Number of workers')\n",
    "ax.set_title('Execution time sliding window correlation\\n(Trials: %1.0f)' % (113))\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(worker_no)\n",
    "plt.savefig(path + os.path.sep + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
