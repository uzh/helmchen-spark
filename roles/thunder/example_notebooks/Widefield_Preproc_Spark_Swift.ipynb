{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess widefield calcium imaging data using Spark\n",
    "This notebook demonstrates how to read binary raw data files stored on UZH Swift object storage into a Spark RDD, convert it into a Numpy array and perform preprocessing to generate a DFF array. Both the raw data and DFF arrays are stored as output HDF5 files on the Swift object storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "from __future__ import print_function\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from scipy.io import savemat\n",
    "import getpass\n",
    "import h5py\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# the notebook backend: 'local' or 'openstack'\n",
    "nbBackend = 'openstack'\n",
    "\n",
    "# add folder 'utils' to the Python path\n",
    "# this folder contains custom written code that is required for data import and analysis\n",
    "utils_dir = os.path.join(os.getcwd(), 'utils')\n",
    "sys.path.append(utils_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import custom-written modules\n",
    "import SwiftStorageUtils\n",
    "import WidefieldDataUtils as wf\n",
    "import BehaviourAnalysisUtils\n",
    "import CalciumAnalysisUtils as calciumTools\n",
    "import parseDCIMGheader as parseDCIMGheader\n",
    "from SwiftStorageUtils import uploadItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start of name for matching files\n",
    "filename_start = '20170214_' # all files with names starting like this will be processed\n",
    "\n",
    "# behaviour log file\n",
    "behaviour_log = '2869L1_gordito01_b20170214.txt'\n",
    "\n",
    "# swift file system\n",
    "swift_container = 'dayra' # specify name of container in Swift (do not use _ etc. in container names!)\n",
    "swift_provider = 'SparkTest' # in general, this should not change\n",
    "\n",
    "# derive the Swift base URI\n",
    "swift_basename = \"swift://\" + swift_container + \".\" + swift_provider + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OpenStack credentials for accessing Swift storage\n",
    "os_username = 'hluetc'\n",
    "os_tenant_name = 'helmchen.hifo.uzh'\n",
    "os_auth_url = 'https://cloud.s3it.uzh.ch:5000/v2.0'\n",
    "# provide OS password\n",
    "os_password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put all these parameters in a dictionary, so that we can pass them conveniently to functions\n",
    "file_params = dict()\n",
    "file_params['filename_start'] = filename_start\n",
    "file_params['swift_container'] = swift_container\n",
    "file_params['swift_provider'] = swift_provider\n",
    "file_params['swift_basename'] = swift_basename\n",
    "file_params['os_username'] = os_username\n",
    "file_params['os_tenant_name'] = os_tenant_name\n",
    "file_params['os_auth_url'] = os_auth_url\n",
    "file_params['os_password'] = os_password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of stimuli and appropriate decisions\n",
    "stim_decision = [\n",
    "    ['Texture 1 P100', 'Go'],\n",
    "    ['Texture 7 P1200', 'No Go']\n",
    "    ]\n",
    "\n",
    "# image dimensions for analysis (aspect ratio MUST be preserved)\n",
    "dims_analysis = (256,256) # use None to skip resizing\n",
    "\n",
    "# sampling rate and trial times\n",
    "sample_rate = 20.0 # Hz\n",
    "t_stim = -1.9 # stimulus cue (auditory)\n",
    "t_textIn = 0 # texture in (i.e. stimulus onset)\n",
    "t_textOut = 2 # texture starting to move out (stimulus offset)\n",
    "t_response = 4.9 # response cue for licking (auditory)\n",
    "t_base = -2 # baseline end (for F0 calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bg_smooth = 30 # SD of Gaussian smoothing kernel for background estimation (in pixel) \n",
    "\n",
    "seg_cutoff = 0.0002 # Segmentation threshold; larger value = bigger mask; \n",
    "# smaller value = smaller mask (i.e. more pixels ignored); suggested = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import behaviour log and analyse performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download and import behaviour log file\n",
    "# local storage directory --> remember to delete afterwards\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# download options\n",
    "down_opts = {\n",
    "    'skip_identical': True,\n",
    "    'out_directory': temp_dir,\n",
    "}\n",
    "\n",
    "from SwiftStorageUtils import downloadItems\n",
    "downloadItems(swift_container, [behaviour_log], file_params, down_opts)\n",
    "\n",
    "from BehaviourAnalysisUtils import parseBehaviourLog\n",
    "trial_list = parseBehaviourLog('%s%s%s' % (temp_dir, os.path.sep, behaviour_log), print_table=False)\n",
    "\n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyse performance\n",
    "from BehaviourAnalysisUtils import analyzeBehaviourPerformance\n",
    "go_trials, nogo_trials, corr_response, corr_reject, miss_response, false_alarm = \\\n",
    "analyzeBehaviourPerformance(trial_list, stim_decision, print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save behaviour performance on Swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write performance to temp. file and push to Swift\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "perf_file = temp_dir + os.path.sep + 'BehaviourPerformance.txt'\n",
    "with open(perf_file, 'w') as fid:\n",
    "    fid.write('Go trials (%s): %1.0f\\n' % ([a[0] for a in stim_decision if a[1] == 'Go'][0], go_trials))\n",
    "    fid.write('No Go trials (%s): %1.0f\\n' % ([a[0] for a in stim_decision if a[1] == 'No Go'][0], nogo_trials))\n",
    "    fid.write('Correct responses: %1.0f\\n' % (corr_response))\n",
    "    fid.write('Correct rejects: %1.0f\\n' % (corr_reject))\n",
    "    fid.write('Missed responses: %1.0f\\n' % (miss_response))\n",
    "    fid.write('False alarms: %1.0f\\n' % (false_alarm))\n",
    "from SwiftStorageUtils import uploadItems\n",
    "uploadItems(swift_container, '', temp_dir, [perf_file], file_params)\n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save trial list as text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write performance to temp. file and push to Swift\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "trial_list_file = temp_dir + os.path.sep + 'TrialList.txt'\n",
    "with open(trial_list_file, 'w') as fid:\n",
    "        for i_trial in trial_list:\n",
    "            fid.write('%1.0f\\t%1.0f\\t%s\\t%s\\t%s\\n' % \n",
    "                      (i_trial[0], i_trial[1], str(i_trial[2]), i_trial[3], i_trial[4]))\n",
    "from SwiftStorageUtils import uploadItems\n",
    "uploadItems(swift_container, '', temp_dir, [trial_list_file], file_params)\n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startSparkContext(max_cores=16):\n",
    "    from setupSpark import initSpark\n",
    "    \n",
    "    executor_cores = 8 # the number of cores to be used on each worker\n",
    "    executor_memory = '25G' # the amount of memory to be used on each worker\n",
    "    max_cores = max_cores # the max. number of cores Spark is allowed to use overall\n",
    "\n",
    "    # returns the SparkContext object 'sc' which tells Spark how to access the cluster\n",
    "    sc = initSpark(nbBackend, executor_cores=executor_cores, \\\n",
    "                   max_cores=max_cores, executor_memory=executor_memory)\n",
    "    \n",
    "    # provide OpenStack credentials to the Spark Hadoop configuration\n",
    "    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.username', os_username)\n",
    "    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.tenant', os_tenant_name)\n",
    "    sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.password', os_password)\n",
    "    \n",
    "    # add Python files in 'utils' folder to the SparkContext \n",
    "    # this is required so that all files are available on all the cluster workers\n",
    "    for filename in os.listdir(utils_dir):\n",
    "        if filename.endswith('.py'):\n",
    "            sc.addPyFile(os.path.join(utils_dir, filename))\n",
    "            \n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = startSparkContext(max_cores=8)\n",
    "time.sleep(10) # wait till setup completes\n",
    "print(\"Parallelism: %1.0f\" % (sc.defaultParallelism))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of relevant binary files\n",
    "from SwiftStorageUtils import listItems\n",
    "container_items = listItems(file_params['swift_container'], file_params)\n",
    "binary_files = [a for a in container_items if a.startswith(file_params['filename_start'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "binary_file_rdd = sc.parallelize(binary_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readDCAMfromSwift(file_name, swift_container, file_params):\n",
    "    \"\"\"\n",
    "    Download binary file from Swift\n",
    "    \"\"\"\n",
    "\n",
    "    # local storage directory --> remember to delete afterwards\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "    # download options\n",
    "    down_opts = {\n",
    "        'skip_identical': True,\n",
    "        'out_directory': temp_dir,\n",
    "    }\n",
    "\n",
    "    from SwiftStorageUtils import downloadItems\n",
    "    downloadItems(swift_container, [file_name], file_params, down_opts)\n",
    "    \n",
    "    path_to_local_file = os.path.join(temp_dir, file_name)\n",
    "    \n",
    "    with open(path_to_local_file, mode='rb') as fid:\n",
    "        byte_stream = fid.read()\n",
    "\n",
    "    # delete temp dir\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "    return byte_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertDCAMtoMov(byte_stream):\n",
    "    \"\"\"\n",
    "    Convert raw DCAM byte-stream to movie. \n",
    "    \n",
    "    Note that parameters (e.g. dims_analysis) are provided as global variables in the notebook.\n",
    "    Image dimensions are obtained by parsing the file header.\n",
    "    \"\"\"\n",
    "    # parse the header and get image dimensions\n",
    "    hdr = parseDCIMGheader.main(byte_stream)\n",
    "    dims = [hdr['xsize'], hdr['ysize'], hdr['nframes']]\n",
    "    \n",
    "    byte_stream = byte_stream[232:] # 232 bytes is the file header\n",
    "    A = np.fromstring(byte_stream, dtype=np.uint16)\n",
    "    A = A[:dims[0]*dims[1]*dims[2]] # remove data points at the end\n",
    "    \n",
    "    # re-arrange data into the correct shape\n",
    "    mov = np.fliplr(A.reshape([dims[0], dims[1], dims[2]], order='F'))\n",
    "    # hack to remove strange pixels with very high intensity\n",
    "    mov[np.where(mov > 60000)] = 0\n",
    "    \n",
    "    # resize to analysis dimensions\n",
    "    mov = wf.resizeMovie(mov, resolution=dims_analysis, interp='bilinear')\n",
    "    \n",
    "    return mov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test with without Spark\n",
    "first_file = binary_file_rdd.first()\n",
    "byte_stream = readDCAMfromSwift(first_file, file_params['swift_container'], file_params)\n",
    "mov = convertDCAMtoMov(byte_stream)\n",
    "plt.imshow(mov[:,:,0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create byte-stream RDD\n",
    "# This will download the binary file and read it from local disk\n",
    "byte_stream_rdd = binary_file_rdd.map(lambda v: (v, readDCAMfromSwift(v, file_params['swift_container'], \n",
    "                                                                          file_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the byte-stream RDD to a numpy array\n",
    "mov_rdd = byte_stream_rdd.map(lambda (k,v): (k, convertDCAMtoMov(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read binary file content directly from Swift using Spark's binaryFiles reader\n",
    "# This part turned out to be relatively error-prone\n",
    "# file_rdd = sc.binaryFiles(file_params['swift_basename'], minPartitions=100)\n",
    "# file_rdd = file_rdd.filter(lambda (k,v): file_params['filename_start'] in k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get first movie (return key-value tuple)\n",
    "mov1 = mov_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the data has been imported correctly, display some frames as images. This will also produce an average image of the first movie (avg). This will be used below to create the reference image as mat file (refImg.mat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path, file_id = os.path.split(mov1[0])\n",
    "print('File: %s' % (file_id))\n",
    "dat = mov1[1]\n",
    "avg = np.nanmean(dat, axis=2)\n",
    "xy = (dat.shape[0]/1.05, dat.shape[1] - (dat.shape[1]/1.1))\n",
    "f, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(dat[:,:,0], cmap='gray', interpolation='none')\n",
    "axes[0].annotate('Frame %1.0f' % 0, xy=xy, fontsize=14, color='yellow', horizontalalignment='right')\n",
    "axes[1].imshow(avg, cmap='gray', interpolation='none')\n",
    "axes[1].annotate('Mean', xy=xy, fontsize=14, color='yellow', horizontalalignment='right')\n",
    "axes[2].imshow(np.nanmax(dat, axis=2), cmap='gray', interpolation='none')\n",
    "axes[2].annotate('Max', xy=xy, fontsize=14, color='yellow', horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of frames from the first movie. Setup the time axis. Specify frames for F0 calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timepoints = dat.shape[2]\n",
    "t = (np.array(range(timepoints)) / sample_rate) - 3.0\n",
    "\n",
    "# Frames for F0 calculation\n",
    "f0_frames = t<t_base # F0 as time before baseline\n",
    "\n",
    "f0_frames[:] = False\n",
    "f0_frames[9:12] = True # F0 as certain specified frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess movie\n",
    "The preprocessing pipeline currently consists of 3 steps: estimation and subtraction of background, segmentation of area of interest, normalization (dF/F calculation). As for conversion, we first define a function that is then applied to the Spark RDD. These transformations are only registered, not executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocMovie(mov, bg_smooth=bg_smooth, seg_cutoff=seg_cutoff):\n",
    "    \"\"\"\n",
    "    Perform preprocessing steps for a movie. \n",
    "    \"\"\"\n",
    "    \n",
    "    # estimate background signal intensity\n",
    "    print('Estimating background', end=\"\")\n",
    "    bg_estimate = wf.estimateBackground(mov[:,:,0], bg_smooth)\n",
    "    print(' - Done (%1.2f)' % bg_estimate)\n",
    "    \n",
    "    # subtract the background (set negative to 0)\n",
    "    mov = mov - bg_estimate\n",
    "    mov[mov<0] = 0\n",
    "    \n",
    "    # segment out the background (set to np.nan)\n",
    "    print('Segmenting background', end=\"\")\n",
    "    mov = wf.segmentBackground(mov, seg_cutoff, plot=False)\n",
    "    print(' - Done')\n",
    "    \n",
    "    # baseline normalization (Dff)\n",
    "    print('Calculating Dff', end=\"\")\n",
    "    dff = calciumTools.calculateDff(mov , f0_frames)\n",
    "    print(' - Done')\n",
    "    \n",
    "    return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply transformation to the RDD\n",
    "dff_rdd = mov_rdd.map(lambda (k,v): (k, preprocMovie(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as HDF5 files\n",
    "Now we can save the data back to the Swift storage. This will finally kick-off the whole processing pipeline that has been defined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the names for the output folders\n",
    "output_folder_mov = 'mov_out'\n",
    "output_folder_dff = 'dff_out'\n",
    "output_folder_mat = 'mat_out'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the folders exist already. If a folder exists, will display the contents and ask for confirmation to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SwiftStorageUtils import deleteExistingFolder\n",
    "deleteExistingFolder(swift_container, output_folder_mov, file_params, confirm=False)\n",
    "deleteExistingFolder(swift_container, output_folder_dff, file_params, confirm=False)\n",
    "deleteExistingFolder(swift_container, output_folder_mat, file_params, confirm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFileNameFromKey(key):\n",
    "    \"\"\"\n",
    "    Return the file name from the RDD key (i.e. split of the swift URL)\n",
    "    \"\"\"\n",
    "    path, name = os.path.split(key)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the image data as HDF5 on Swift storage. \n",
    "# This will run all the transformations that have been registered for mov_rdd.\n",
    "from SwiftStorageUtils import saveAsH5\n",
    "mov_rdd.foreach(lambda (k,v): (k, saveAsH5(v, getFileNameFromKey(k), 'mov', output_folder_mov, file_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the dFF data as HDF5 on Swift storage. \n",
    "# This will run all the transformations that have been registered for dff_rdd.\n",
    "dff_rdd.foreach(lambda (k,v): (k, saveAsH5(v, getFileNameFromKey(k), 'dff', output_folder_dff, file_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Save mat-files for OCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the image data as Matlab mat-file on Swift storage.\n",
    "from SwiftStorageUtils import saveAsMat\n",
    "dff_rdd.foreach(lambda (k,v): (k, saveAsMat(v, getFileNameFromKey(k), 'tr', output_folder_mat, file_params, \n",
    "                                            trial_list=trial_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop SparkContext to free cluster resources\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create trials_ind.mat and ref_img.mat for processing with OCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a temporary directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "if temp_dir.endswith(os.path.sep):\n",
    "    pass\n",
    "else:\n",
    "    temp_dir = temp_dir + os.path.sep\n",
    "\n",
    "# Create trials_ind.mat\n",
    "# TODO: make it work with one or more than 2 stimulus types\n",
    "stim_type_ids = [[],[]]\n",
    "for i_trial in trial_list:\n",
    "    if i_trial[3] == stim_decision[0][0]:\n",
    "        stim_type_ids[0].append(i_trial[0])\n",
    "    elif i_trial[3] == stim_decision[1][0]:\n",
    "        stim_type_ids[1].append(i_trial[0])\n",
    "    else:\n",
    "        raise ValueError('Stim type %s unknown!' % (i_trial[3]))\n",
    "var_dict = dict()\n",
    "for ix, i_id in enumerate(stim_type_ids):\n",
    "    trial_stim = stim_decision[ix][0]\n",
    "    dataset_name = \"tr_%s\" % (trial_stim[trial_stim.rfind(' ')+2:])\n",
    "    var_dict[dataset_name] = i_id\n",
    "matfile = os.path.join(temp_dir, 'trials_ind.mat')\n",
    "\n",
    "# save mat-file to temp dir\n",
    "savemat(matfile, var_dict)\n",
    "uploadItems(file_params['swift_container'], output_folder_mat, temp_dir, [matfile], file_params)\n",
    "\n",
    "# Process reference image (average of first movie) and save as mat\n",
    "matfile = os.path.join(temp_dir, 'refImg.mat')\n",
    "\n",
    "# scale between 0 and 0.1 (roughly)\n",
    "avg = (avg / np.nanmax(avg)) / 10\n",
    "# put in dict and save\n",
    "var_dict = dict()\n",
    "var_dict['refImg'] = avg\n",
    "savemat(matfile, var_dict, do_compression=True)\n",
    "uploadItems(file_params['swift_container'], output_folder_mat, temp_dir, [matfile], file_params)\n",
    "\n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
