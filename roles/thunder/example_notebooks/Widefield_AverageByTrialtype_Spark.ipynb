{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average arrays by trial type\n",
    "This notebook demonstrates how to average single-trial data (from Widefield_Preproc_Spark_Swift, Widefield_Correlation_Spark) by trial type. The output is stored as movies per trial type on Swift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import pylab as plt\n",
    "from __future__ import print_function\n",
    "import getpass\n",
    "import tempfile\n",
    "import shutil\n",
    "import h5py\n",
    "import re\n",
    "import csv\n",
    "%matplotlib inline\n",
    "\n",
    "nbBackend = 'openstack'\n",
    "\n",
    "# add folder 'utils' to the Python path\n",
    "# this folder contains custom written code that is required for data import and analysis\n",
    "utils_dir = os.path.join(os.getcwd(), 'utils')\n",
    "sys.path.append(utils_dir)\n",
    "import SwiftStorageUtils\n",
    "from SwiftStorageUtils import saveAsMat\n",
    "import WidefieldDataUtils as wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# swift file system parameters\n",
    "swift_container = 'dayra' # specify name of container in Swift (do not use _ etc.)\n",
    "swift_provider = 'SparkTest' # in general, this should not change\n",
    "swift_basename = \"swift://\" + swift_container + \".\" + swift_provider + \"/\"\n",
    "\n",
    "# storage location of data relative to swift_basename\n",
    "data_folder = 'mov_out'\n",
    "\n",
    "# array name in the HDF5 file (either mov, dff or corr)\n",
    "h5_array_id = 'mov'\n",
    "\n",
    "# start of name for matching files\n",
    "filename_start = '20170214_' # all files with names starting like this will be processed\n",
    "\n",
    "# save mat-file output\n",
    "save_mat = True\n",
    "# in which folder to save the mat files\n",
    "mat_folder = 'mat_out'\n",
    "\n",
    "# save a movie animation (mp4) in the folder animations\n",
    "save_movie = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OpenStack credentials for accessing Swift storage\n",
    "os_username = 'hluetc'\n",
    "os_tenant_name = 'helmchen.hifo.uzh'\n",
    "os_auth_url = 'https://cloud.s3it.uzh.ch:5000/v2.0'\n",
    "# provide OS password\n",
    "os_password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put all these params in a dict for later access\n",
    "file_params = dict()\n",
    "file_params['swift_container'] = swift_container\n",
    "file_params['swift_provider'] = swift_provider\n",
    "file_params['swift_basename'] = swift_basename\n",
    "file_params['os_username'] = os_username\n",
    "file_params['os_tenant_name'] = os_tenant_name\n",
    "file_params['os_auth_url'] = os_auth_url\n",
    "file_params['os_password'] = os_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from setupSpark import initSpark\n",
    "# Initialize Spark\n",
    "# specify the number of cores and the memory of the workers\n",
    "# each worker VM has 8 cores and 32 GB of memory\n",
    "# the status of the cluster (ie. how many cores are available) can be checked in the Spark UI:\n",
    "# http://SparkMasterIP:8080/\n",
    "\n",
    "spark_instances = 2 # the number of workers to be used\n",
    "executor_cores = 8 # the number of cores to be used on each worker\n",
    "executor_memory = '28G' # the amount of memory to be used on each worker\n",
    "max_cores = spark_instances*executor_cores # the max. number of cores Spark is allowed to use overall\n",
    "\n",
    "# returns the SparkContext object 'sc' which tells Spark how to access the cluster\n",
    "sc = initSpark(nbBackend, spark_instances=spark_instances, executor_cores=executor_cores, \\\n",
    "               max_cores=max_cores, executor_memory=executor_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# provide OS credentials to the Hadoop configuration\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.username', os_username)\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.tenant', os_tenant_name)\n",
    "sc._jsc.hadoopConfiguration().set('fs.swift.service.SparkTest.password', os_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add Python files in 'utils' folder to the SparkContext \n",
    "# this is required so that all files are available on all the cluster workers\n",
    "for filename in os.listdir(utils_dir):\n",
    "    if filename.endswith('.py'):\n",
    "        sc.addPyFile(os.path.join(utils_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get relevant files from container and create RDD from list of objects to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SwiftStorageUtils import listItems\n",
    "object_list = listItems(swift_container, file_params)\n",
    "objects_to_download = [n for n in object_list if n.startswith(data_folder) and filename_start in n]\n",
    "objects_to_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_rdd = sc.parallelize(objects_to_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import trial IDs and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read trials_ind.mat file (deprecated)\n",
    "# File with trial indices\n",
    "# trials_index_file = 'trials_ind.mat'\n",
    "\n",
    "# # local storage directory --> remember to delete afterwards\n",
    "# temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# # download options\n",
    "# down_opts = {\n",
    "#     'skip_identical': True,\n",
    "#     'out_directory': temp_dir,\n",
    "# }\n",
    "\n",
    "# from SwiftStorageUtils import downloadItems\n",
    "# downloadItems(swift_container, [trials_index_file], file_params, down_opts)\n",
    "\n",
    "# trial_ind = wf.importTrialIndices('%s%s%s' % (temp_dir, os.path.sep, trials_index_file))\n",
    "\n",
    "# # delete temp dir\n",
    "# shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read TrialList.txt (created during pre-processing)\n",
    "# File with trial information\n",
    "trial_list_file = 'TrialList.txt'\n",
    "\n",
    "# local storage directory --> remember to delete afterwards\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# download options\n",
    "down_opts = {\n",
    "    'skip_identical': True,\n",
    "    'out_directory': temp_dir,\n",
    "}\n",
    "\n",
    "from SwiftStorageUtils import downloadItems\n",
    "downloadItems(swift_container, [trial_list_file], file_params, down_opts)\n",
    "\n",
    "with open('%s%s%s' % (temp_dir, os.path.sep, trial_list_file)) as fid:\n",
    "    reader=csv.reader(fid,delimiter='\\t')\n",
    "    trial_list = []\n",
    "    for line in reader:\n",
    "        line[0] = int(line[0])\n",
    "        line[1] = int(line[1])\n",
    "        trial_list.append(line)\n",
    "\n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrialType(selected_file, trial_list, data_folder):\n",
    "    \"\"\"\n",
    "    Return trial type of input file using trial list lookup\n",
    "    \"\"\"\n",
    "    # parse file name for trial no.\n",
    "    selected_file = selected_file.replace(data_folder + '/', '').replace('.h5', '')\n",
    "    trial_no = int(selected_file[selected_file.rfind('_')+1:])\n",
    "    # search trial list to find trial type\n",
    "    trial_type = [i for i in trial_list if i[0] == trial_no][0][3]\n",
    "    # TODO: filter correct / incorrect trials\n",
    "    trial_type = trial_type[trial_type.rfind(' ')+1:]\n",
    "    return trial_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the key of each RDD element as the trial type. Check the first few elements to see if results make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set trial_type as key\n",
    "file_rdd_keyed = file_rdd.map(lambda x: (getTrialType(x, trial_list, data_folder), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_rdd_keyed.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a dictionary with unique trials and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trial_count = file_rdd_keyed.countByKey()\n",
    "trial_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "Next, we import the data from HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getArrayFromH5(h5file, dataset_name):\n",
    "    \"\"\"\n",
    "    Return array data stored in HDF5 file\n",
    "    \"\"\"\n",
    "    with h5py.File(h5file,'r') as hf:\n",
    "        print('List of arrays in HDF5 file: ', hf.keys())\n",
    "        data = hf.get(dataset_name)\n",
    "        data = np.array(data)\n",
    "        print('Shape of the array %s: ' % (dataset_name), data.shape)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SwiftStorageUtils import downloadItems\n",
    "def convert2rdd(obj, file_params, h5_array_id):\n",
    "    \"\"\"\n",
    "    Import HDF5 array data into Spark RDD\n",
    "    \"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    # download options\n",
    "    down_opts = {\n",
    "        'skip_identical': True,\n",
    "        'out_directory': temp_dir,\n",
    "    }\n",
    "    downloadItems(file_params['swift_container'], [obj], file_params, down_opts)\n",
    "    \n",
    "    local_file = '%s%s%s' % (temp_dir, os.path.sep, obj)\n",
    "    print('Local file: ', local_file)\n",
    "    \n",
    "    data = getArrayFromH5(local_file, h5_array_id)\n",
    "    \n",
    "    # delete temp dir\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup RDD as Key-Value pair with key=trial_type and value as (data_array, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trial_arr_rdd = file_rdd_keyed.map(lambda (k,v): (k, (convert2rdd(v, file_params, h5_array_id), 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first data array to determine number of timepoints and calculate time vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr1 = trial_arr_rdd.first()[1][0]\n",
    "timepoints = arr1.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time vector\n",
    "sample_rate = 20.0 # Hz\n",
    "t = (np.array(range(timepoints)) / sample_rate) - 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the average array over trials\n",
    "This is done in two steps: first add up the arrays for each trial type, keeping track of the number of trials. Second, divide the summed array by the number of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduceByKey will add up arrays of a specific trial_type (key) \n",
    "# and also keep track of the number of trials per trial type\n",
    "avg_rdd = trial_arr_rdd.reduceByKey(lambda (arr1, count1), (arr2, count2): (arr1+arr2, count1+count2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# divide the final summed array per trial_type by the number of trials to get the average\n",
    "avg_rdd = avg_rdd.map(lambda (k,v): (k, (v[0]/v[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save average as mat / movie\n",
    "For each trial type, compute the average and then save the movie back to Swift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i_trial in trial_count:\n",
    "    print(\"Computing average for trial type %s\" % (i_trial))\n",
    "    trial_avg = avg_rdd.filter(lambda (k,v): k==i_trial).first()[1]\n",
    "    if save_mat:\n",
    "        print(\"Saving mat-file for trial type %s\" % (i_trial))\n",
    "        matfile_name = 'cond_%s_AVG.mat' % (i_trial)\n",
    "        dataset_name = '%s_AVG' % (i_trial)\n",
    "        saveAsMat(trial_avg, matfile_name, dataset_name, mat_folder, file_params)\n",
    "    if save_movie:\n",
    "        print(\"Saving movie for trial type %s\" % (i_trial))\n",
    "        wf.saveMovie(trial_avg, i_trial, h5_array_id, sample_rate, t, file_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
