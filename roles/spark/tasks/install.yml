---
# Install spark
- block: 
  - name: download spark
    get_url:
      url=http://mirror.easyname.ch/apache/spark/spark-{{spark_version}}/spark-{{spark_version}}-bin-hadoop2.7.tgz  
      dest=/usr/local/spark-{{spark_version}}-bin-hadoop2.7.tgz

  - name: unzip spark
    unarchive: 
      copy=no 
      src=/usr/local/spark-{{spark_version}}-bin-hadoop2.7.tgz 
      dest=/usr/local/
      owner={{ hadoop_user }}
      group=hadoop 
      creates=/usr/local/spark-{{spark_version}}-bin-hadoop2.7
    
  - name: make spark symlink
    file: 
      src=/usr/local/spark-{{spark_version}}-bin-hadoop2.7 
      dest=/usr/local/spark state=link 
      owner={{ hadoop_user }}
      group=hadoop 

  tags:
    - downloads

- block: 
  - name: deploy slaves configuration
    template: src=templates/slaves.j2 dest=/usr/local/spark/conf/slaves 

  - name: deploy spark-env.sh configuration
    template: src=templates/spark-env.sh.j2 dest=/usr/local/spark/conf/spark-env.sh owner={{ hadoop_user }} group=hadoop
    notify: 
      - restart spark master
      - restart spark workers
  
  - name: install findspark
    pip: name=findspark state=present

  - name: add spark environment variables to bashrc
    become: yes
    become_user: "{{ item }}"
    lineinfile:
      dest=~/.bashrc 
      state=present insertafter=EOF 
      line="export SPARK_HOME=/usr/local/spark; export PYSPARK_PYTHON=/usr/local/miniconda/bin/python"
      create=true
    with_items: 
      - "{{ core_users }}"
      - "{{ regular_users }}"
      - "root"
  
  tags: 
    - configuration

- name: print groups
  debug: var=hostvars[inventory_hostname]
