---
# hadoop version
hadoop_version: 2.7.2

# spark version 
spark_version: 2.1.1
# spark_version: 1.6.1

# Set to absent to take down machines
instance_state: present

# Number of workers to spawn in the cluster
nbr_of_slaves: 4

# VM flavor
vm_flavor: 1cpu-4ram-hpc

# The name of the key pair you will use to log in
# as set in OpenStack (see the OpenStack security dashboard)
key_name: neurospark

# Id of the network to run in (can be found in the OpenStack
# dashboard)
network_id: c86b320c-9542-4032-a951-c8a068894cc2

# This is the ssh-key which will be distritbuted 
# across the cluster. It' important that this key
# does NOT use password protection
ssh_keys_to_use: files/spark-nodes.key
ssh_public_key: files/spark-nodes.key.pub
# This is a ugly hack - but you have to give the
# file name of the ssh-key your referencing above
name_of_ssh_key: spark-nodes.key

# Name of the user used to install everything
# on the remote systems
user: ubuntu

# Name of hadoop user
hadoop_user: hadoop

# make sure we are ssh-ing as the ubuntu user -- this is
# important when running playbooks remotely
ansible_user: ubuntu 

core_users:
  - "{{ user }}"
  - "{{ hadoop_user }}"
  
# HDFS options

# whether to reformat HDFS 
# be very careful if you set this to true, you probably only want 
# to do it during the first initialization
hdfs_reformat: false

# hadoop directory location - this is where all "temporary" data, 
# such as HDFS blocks, user data, mapreduce temporary data etc. goes
hadoop_tmp_dir: /hadoop

# hadoop configuration directory
hadoop_conf_dir: /usr/local/hadoop/etc/hadoop

# hadoop environment variables
hadoop_env:
  HADOOP_LOGFILE: hadoop.log
  HADOOP_DATANODE_OPTS: -Dhadoop.security.logger=ERROR,RFAS
  HADOOP_LOG_DIR: /usr/local/hadoop-2.7.2/logs
  HADOOP_IDENT_STRING: hadoop
  HADOOP_PREFIX: /usr/local/hadoop-2.7.2
  HADOOP_POLICYFILE: hadoop-policy.xml
  HADOOP_COMMON_LIB_JARS_DIR: share/hadoop/common/lib
  HADOOP_COMMON_LIB_NATIVE_DIR: lib/native
  HADOOP_HDFS_HOME: /usr/local/hadoop-2.7.2
  HADOOP_CLIENT_OPTS: -Xmx512m
  HADOOP_COMMON_HOME: /usr/local/hadoop-2.7.2
  HADOOP_YARN_HOME: /usr/local/hadoop-2.7.2
  HADOOP_CLASSPATH: /contrib/capacity-scheduler/*.jar
  HADOOP_CONF_DIR: /usr/local/hadoop-2.7.2/etc/hadoop
  HADOOP_PORTMAP_OPTS: -Xmx512m
  HADOOP_OPTS:  -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/usr/local/hadoop-2.7.2/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/local/hadoop-2.7.2 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Djava.library.path=/usr/local/hadoop-2.7.2/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true
  HADOOP_COMMON_DIR: share/hadoop/common
  HADOOP_SECONDARYNAMENODE_OPTS: -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender
  HADOOP_DEFAULT_PREFIX: /usr/local/hadoop-2.7.2
  HADOOP_NAMENODE_OPTS: -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender
  HADOOP_MAPRED_HOME: /usr/local/hadoop-2.7.2
  HADOOP_LOGLEVEL: INFO
