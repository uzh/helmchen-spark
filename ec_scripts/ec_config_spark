# Elasticluster Configuration Template
# ====================================
#
# This is a test configuration file for elasticluster.
#
# The file is parsed by ConfigParser module and has a syntax similar
# to Microsoft Windows INI files.
#
# It consists of `sections` led by a `[sectiontype/name]` header and
# followed by lines in the form
#
# key=value
#
# Section names are in the form `[type/name]` wher `type` must be one of:
#
# - cloud   (define a cloud provider)
# - login   (define a way to access a virtual machine)
# - setup   (define a way to setup the cluster)
# - cluster (define the composition of a cluster. It contains
#            references to the other sections.)
# - cluster/<clustername>
#            (override configuration for specific group
#            of nodes within a cluster)
#
#
# You must define at least one for each section types in order to have
# a valid configuration file.

# Cloud Section
# =============
#
# A `cloud` section named `<name>` starts with:
#
#
# The cloud section defines all properties needed to connect to a
# specific cloud provider.
#
# You can define as many cloud sections you want, assuming you have
# access to different cloud providers and want to deploy different
# clusters in different clouds. The mapping between cluster and cloud
# provider is done in the `cluster` section (see later).
#
# Currently two cloud providers are available:
# - ec2_boto:  OpenStack and Amazon EC2
# - google:    Google Compute Engine
# - openstack: OpenStack native API
#
# Therefore the following configuration option needs to be set in the cloud
# section:
# provider: the driver to use to connect to the cloud provider.
#           `ec2_boto` or `google`
#

# Valid configuration keys for *openstack*
# ----------------------------------------
#
# auth_url:          The URL of the keystone service (main entry point for
#                    OpenStack clouds), same as option --os-auth-url
#                    of `nova` command. If an environment variable
#                    `OS_AUTH_URL` is set, this option is ignored.
#
# username:          OpenStack username, same as option --os-username of
#                    `nova` command. If an environment variable
#                    `OS_USERNAME` is set, this option is ignored.
#
# password:          OpenStack password, same as option --os-password of
#                    `nova` command. If an environment variable
#                    `OS_PASSWORD` is set, this option is ignored.
#
# project_name:      OpenStack project to use (also known as `tenant`),
#                    same as option `--os-tenant-name` of `nova`
#                    command. If an environment variable
#                    `OS_TENANT_NAME` is set, this option is ignored.
#
# region_name:       OpenStack region (optional)
# request_floating_ip: request assignment of a floating IP when the
#                      instance is started.
#                      Valid values: `True`, `False`. Default is `False`
#

[cloud/uzh]
provider=openstack
auth_url=https://cloud.s3it.uzh.ch:5000/v2.0
username=hl
password=pa
project_name=he
# request_floating_ip=False
# region_name=


# Login Section
# ===============
#
# A `login` section named `<name>` starts with:
#
# [login/<name>]
#
# This section contains information on how to access the instances
# started on the cloud, including the user and the SSH keys to use.
#
# Some of the values depend on the image you specified in the
# `cluster` section. Values defined here also can affect the `setup`
# section and the way the system is setup.
#
# Mandatory configuration keys
# ----------------------------
#
# image_user: the remote user you must use to connect to the virtual
#             machine. In case you're using Google Compute Engine you have
#             to set your username here. So if your gmail address is
#             karl.marx@gmail.com, your username is karl.marx.
#
# image_sudo: Can be `True` or `False`. `True` means that on the
#             remote machine you can execute commands as root by
#             running the `sudo` program.
#
# image_user_sudo: the login name of the administrator. Use `root`
#                  unless you know what you are doing...
#
# user_key_name: name of the *keypair* to use on the cloud
#                provider. If the keypair does not exist it will be
#                created by elasticluster.
#
# user_key_private: file containing a valid RSA or DSA private key to
#                   be used to connect to the remote machine. Please
#                   note that this must match the `user_key_public`
#                   file (RSA and DSA keys go in pairs). Also note
#                   that Amazon does not accept DSA keys but only RSA
#                   ones.
#
# user_key_public: file containing the RSA/DSA public key
#                  corresponding to the `user_key_private` private
#                  key. See `user_key_private` for more details.
#
#
# For a typical ubuntu machine, both on Amazon and most OpenStack
# providers, these values should be fine:

[login/ubuntu]
image_user=ubuntu
image_user_sudo=root
image_sudo=True
user_key_name=neurosparkcluster
user_key_private=~/.ssh/id_rsa
user_key_public=~/.ssh/id_rsa.pub


# Setup Section
# =============
#
# A `setup` section named `<name>` starts with:
#
# [setup/<name>]
#
# This section contain information on *how to setup* a cluster. After
# the cluster is started, elasticluster will run a `setup provider` in
# order to configure it.
#
# Mandatory configuration keys
# ----------------------------
#
# provider: the type of setup provider. So far, only `ansible` is
#           supported.
#
# Ansible-specific mandatory configuration keys
# ----------------------------------------------
#
# The following configuration keys are only valid if `provider` is
# `ansible`.
#
# <class>_groups: Comma separated list of ansible groups the specific
#                 <class> will belong to. For each <class>_nodes in a
#                 [cluster/] section there should be a <class>_groups
#                 option to configure that specific class of nodes
#                 with the ansible groups specified.
#
#                 If you are setting up a standard HPC cluster you
#                 probably want to have only two main groups:
#                 `frontend_groups` and `compute_groups`.
#
#                 An incomplete list of available ansible groups is:
#
#                 - slurm_master: configure this machine as slurm masternode
#
#                 - slurm_clients: compute nodes of a slurm cluster
#
#                 - gridengine_master: configure as gridengine masternode
#
#                 - gridengine_clients: compute nodes of a gridengine cluster
#
#                 - pbs_master,maui_master: configure as torque
#                   server and maui scheduler (please use both groups
#                   together)
#
#                 - pbs_clients: compute nodes of a pbs+maui cluster
#
#                 - ganglia_master: configure as ganglia web frontend.
#                   On the master, you probably want to define
#                   `ganglia monitor` as well
#
#                 - ganglia_monitor: configure as ganglia monitor.
#
#                 - ipython_controller: configure as a controller
#                   (HUB) for an IPython cluster
#
#                 - ipython_engine: configure as an `engine` for an
#                   IPython cluster
#
#                 You can combine more groups together, but of course
#                 not all combinations make sense. A common setup is,
#                 for instance, for `frontend_groups`:
#
#                 slurm_master,ganglia_master,ganglia_monitor
#
# <class>_var_<varname>: an entry of this type will define a variable
#               called <varname> for the specific <class> and add it
#               to the ansible inventory file.
#
# playbook_path: Path to the playbook to use when configuring the
#                system. The default value printed here points to the
#                playbook distributed with elasticluster. Default is
#                to use the playbooks distributed with elasticluster
#
# ssh_pipelining: True|False
#                enable or disable ssh pipelining when setting up the
#                cluster. By default ssh pipelining is enabled, as it
#                improves connection speed. Adding this option to
#                `False` will disable it.

[setup/spark]
provider=ansible
playbook_path=/home/ubuntu/helmchen-spark/spark-hadoop.yml
controller_groups=spark_masters
worker_groups=spark_slaves



# Cluster Section
# ===============
#
# A `cluster` section named `<name>` starts with:
#
# [cluster/<name>]
#
# The cluster section defines a `template` for a cluster. This section
# has references to each one of the other sections and define the
# image to use, the default number of compute nodes and the security
# group.
#
# Mandatory configuration keys
# -----------------------------
#
# cloud: the name of a valid `cloud` section. For instance `hobbes` or
#        `amazon-us-east-1`
#
# login: the name of a valid `login` section. For instance `ubuntu` or
#        `gc3-user`
#
# setup_provider: the name of a valid `setup` section. For instance,
#                 `ansible-slurm` or `ansible-pbs`
#
# image_id: image id in `ami` format. If you are using OpenStack, you
#           need to run `euca-describe-images` to get a valid `ami-*`
#           id.
#
# flavor: the image type to use. Different cloud providers call it
#         differently, could be `instance type`, `instance size` or
#         `flavor`.
#
# security_group: Security group to use when starting the instance.
#
# <class>_nodes: the number of nodes of class `<class>`. These
#                configuration options will define the composition of
#                your cluster. A very common configuration will
#                include only two group of nodes:
#
#                frontend_nodes: the queue manager and frontend of the
#                cluster. You probably want only one.
#
#                compute_nodes: the worker nodes of the cluster.
#
#                Each <class>_nodes group is configured using the
#                corresponding <class>_groups configuration option in
#                the [setup/] section.
#
# ssh_to: `ssh` and `sftp` nodes will connect to only one node. This
#          is the first of the group specified in this configuration
#          option, or the first node of the first group in
#          alphabetical order.  For instance, if you don't set any
#          value for `ssh_to` and you defined two groups:
#          `frontend_nodes` and `compute_nodes`, the ssh and sftp
#          command will connect to `compute001` which is the first
#          `compute_nodes` node. If you specify `frontend`, instead,
#          it will connect to `frontend001` (or the first node of the
#          `frontend` group).
#
# Optional configuration keys
# ---------------------------
#
# image_userdata: shell script to be executed (as root) when the
#                 machine starts. This is usually not needed because
#                 the `ansible` provider works on *vanilla* images,
#                 but if you are using other setup providers you may
#                 need to execute some command to bootstrap it.
#
# network_ids: comma separated list of network IDs the nodes of the
#              cluster will be connected to. Only supported when the
#              cloud provider is ``openstack``
#
# <class>_min_nodes: Define the minimum amount of nodes of type
#          `<class>` that must be up&running to configure the
#          cluster. When starting a cluster, creation of some
#          instances may fail. If at least min_nodes are started
#          correctly (i.e. are not in error state), the cluster is
#          configured anyway, otherwise creation of the cluster will
#          fail.
#
# boot_disk_type: Define the type of boot disk to use.
#                 Only supported when the cloud provider is google.
#                 Supported values are pd-standard and pd-ssd.
#                 Default value is pd-standard.
#
# boot_disk_size: Define the size of boot disk to use.
#                 Only supported when the cloud provider is google.
#                 Values are specified in gigabytes.
#                 Default value is 10.
#
# tags: Comma-separated list of instance tags.
#       Only supported when the cloud provider is `google`.
#
# scheduling: Define the type of instance scheduling.
#             Only supported when the cloud provider is `google`.
#             Only supported value is `preemptible`.

[cluster/spark]
cloud=uzh
login=ubuntu
setup_provider=spark
security_group=spark
# this is the Ubuntu base image on OpenStack (Ubuntu Server 14.04.3 LTS (2016-02-15))
image_id=c18fe24e-210b-47d4-b347-4b0beff4ec50

# this is a snapshot image derived from above but with most required packages already installed and updated
# image_id=908bface-484c-4900-a8d2-1265c3f578d7

flavor=8cpu-32ram-hpc
controller_nodes=1
worker_nodes=2
image_userdata=
ssh_to=controller



# Cluster node section
# ====================
#
# A `cluster node` for the node type `<nodetype>` of the cluster
# `<name>` starts with:
#
# [cluster/<name>/<nodetype>]
#
# This section allows you to override some configuration values for
# specific group of nodes. Assume you have a standard slurm cluster
# with a frontend which is used as manager node and nfs server for the
# home directories, and a set of compute nodes.
#
# You may want to use different flavors for the frontend and the
# compute nodes, since for the first you need more space and you don't
# need many cores or much memory, while the compute nodes may requires
# more memory and more cores but are not eager about disk space.
#
# This is achieved defining, for instance, a `bigdisk` flavor (the
# name is just fictional) for the frontend and `8cpu32g` for the
# compute nodes. Your configuration will thus look like:
#
# [cluster/slurm]
# ...
# flavor=8cpu32g
# frontend_nodes=1
# compute_nodes=10
#
# [cluster/slurm/frontend]
# flavor=bigdisk
